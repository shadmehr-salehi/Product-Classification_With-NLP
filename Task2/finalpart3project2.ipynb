{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64afb4b8-cfe3-487a-be89-594eedcbd9a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (uriginal): https://drive.google.com/uc?id=1-F7f9oSjFx0Si44DvdkzFhjjwfro_Y1p\n",
      "From (redirected): https://drive.google.com/uc?id=1-F7f9oSjFx0Si44DvdkzFhjjwfro_Y1p&confirm=t&uuid=354d38f4-e446-431f-acb1-9ef51fd8fdd5\n",
      "To: /jupyter/train_data.csv\n",
      "100%|██████████| 635M/635M [00:28<00:00, 22.2MB/s] \n",
      "/tmp/ipykernel_274/1005448238.py:10: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train_data_df = pd.read_csv('train_data.csv')\n"
     ]
    }
   ],
   "source": [
    "# Import Data\n",
    "\n",
    "import gdown\n",
    "import pandas as pd\n",
    "\n",
    "# Train Data\n",
    "url = 'https://drive.google.com/uc?id=1-F7f9oSjFx0Si44DvdkzFhjjwfro_Y1p'\n",
    "output = 'train_data.csv'\n",
    "gdown.download(url, output, quiet=False)\n",
    "train_data_df = pd.read_csv('train_data.csv')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2017841-cfc8-4a4d-8c0e-14624a23c118",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-12 03:07:38.354948: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-12 03:07:39.095770: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overall\n",
      "5    56756\n",
      "4    56756\n",
      "3    56756\n",
      "2    56756\n",
      "1    56756\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/_encoders.py:972: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "2023-11-12 03:25:25.324699: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22275 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:09:00.0, compute capability: 8.6\n",
      "2023-11-12 03:39:29.172418: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:606] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-12 03:39:54.571635: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1c4e83da0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-11-12 03:39:54.571676: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3090, Compute Capability 8.6\n",
      "2023-11-12 03:39:54.579098: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-11-12 03:39:54.708978: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
      "2023-11-12 03:39:54.758708: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2023-11-12 03:39:54.822236: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35473/35473 [==============================] - 8304s 233ms/step - loss: 0.9654 - accuracy: 0.5811\n",
      "Epoch 2/3\n",
      "35473/35473 [==============================] - 8207s 231ms/step - loss: 0.8240 - accuracy: 0.6468\n",
      "Epoch 3/3\n",
      "35473/35473 [==============================] - 8189s 231ms/step - loss: 0.7354 - accuracy: 0.6918\n",
      "20974/20974 [==============================] - 1810s 86ms/step\n",
      "F1 Score: 0.05806411918752488\n",
      "20974/20974 [==============================] - 1849s 88ms/step - loss: 0.7972 - accuracy: 0.6711\n",
      "Accuracy: 0.671122670173645\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import f1_score\n",
    "import tensorflow as tf\n",
    "from transformers import TFBertForSequenceClassification, BertTokenizer\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming you have the dataset loaded in train_data_df\n",
    "# Change the path or method according to your dataset loading process\n",
    "# train_data_df = pd.read_csv('your_dataset.csv')\n",
    "# تقسیم داده‌ها به دو بخش آموزش و آزمون\n",
    "train_data, test_data = train_test_split(train_data_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Calculate the number of samples to downsample to for each class\n",
    "num_samples_per_class = 56756\n",
    "\n",
    "# Resample the training data\n",
    "resampled_train_data = pd.concat([\n",
    "    resample(train_data[train_data['overall'] == 5], replace=True, n_samples=num_samples_per_class, random_state=42),\n",
    "    resample(train_data[train_data['overall'] == 4], replace=True, n_samples=num_samples_per_class, random_state=42),\n",
    "    resample(train_data[train_data['overall'] == 3], replace=True, n_samples=num_samples_per_class, random_state=42),\n",
    "    resample(train_data[train_data['overall'] == 2], replace=True, n_samples=num_samples_per_class, random_state=42),\n",
    "    resample(train_data[train_data['overall'] == 1], replace=True, n_samples=num_samples_per_class, random_state=42)\n",
    "])\n",
    "print(resampled_train_data['overall'].value_counts())\n",
    "\n",
    "# استفاده از OneHotEncoder برای تبدیل ستون 'overall' به فرمت one-hot\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "y_train_onehot = encoder.fit_transform(resampled_train_data['overall'].values.reshape(-1, 1))\n",
    "y_test_onehot = encoder.transform(test_data['overall'].values.reshape(-1, 1))\n",
    "\n",
    "# Tokenize the input texts\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', max_length=128)\n",
    "train_encodings = tokenizer(list(resampled_train_data[\"reviewText\"]), truncation=True, padding=True)\n",
    "test_encodings = tokenizer(list(test_data[\"reviewText\"]), truncation=True, padding=True)\n",
    "\n",
    "# Create TensorFlow datasets\n",
    "batch_size = 8\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings), y_train_onehot)).shuffle(len(resampled_train_data)).batch(batch_size)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings), y_test_onehot)).batch(batch_size)\n",
    "\n",
    "# Create the model\n",
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-multilingual-cased', num_labels=5)\n",
    "\n",
    "# Compile the model with accuracy metric\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n",
    "loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_dataset, epochs=3)\n",
    "\n",
    "\n",
    "# Evaluate the model for F1 score\n",
    "predictions = model.predict(test_dataset)\n",
    "predicted_labels = tf.argmax(predictions.logits, axis=1)\n",
    "f1 = f1_score(test_data['overall'].values, predicted_labels, average='weighted')\n",
    "print(\"F1 Score:\", f1)\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "results = model.evaluate(test_dataset)\n",
    "accuracy = results[1]\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b11ba5a-b801-48c5-9056-ef2d5046014f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20974/20974 [==============================] - 1846s 88ms/step - loss: 0.7972 - accuracy: 0.6711\n",
      "Accuracy: 0.671122670173645\n",
      "F1 Score: 0.6911833956308225\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(test_dataset)\n",
    "accuracy = results[1]\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Transform one-hot encoded predictions back to labels\n",
    "predicted_labels = tf.argmax(predictions.logits, axis=1)\n",
    "\n",
    "# Transform one-hot encoded ground truth labels back to labels\n",
    "true_labels = tf.argmax(y_test_onehot, axis=1)\n",
    "\n",
    "# Evaluate the model for F1 score\n",
    "f1 = f1_score(true_labels.numpy(), predicted_labels.numpy(), average='weighted')\n",
    "print(\"F1 Score:\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c08b496a-8956-4e15-881d-ace5bf49f19b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1-8TsrqTRFP-q9TM-6HinhO0ZVXFHq9TB\n",
      "To: /jupyter/test_data.csv\n",
      "100%|██████████| 15.6M/15.6M [00:02<00:00, 6.27MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Test Data\n",
    "url = 'https://drive.google.com/uc?id=1-8TsrqTRFP-q9TM-6HinhO0ZVXFHq9TB'\n",
    "output = 'test_data.csv'\n",
    "gdown.download(url, output, quiet=False)\n",
    "test_data_df = pd.read_csv('test_data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3acb3baf-0750-4636-af20-325207176beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500/2500 [==============================] - 218s 86ms/step\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "# Tokenize the input texts\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', max_length=128)\n",
    "test_encodings = tokenizer(list(test_data_df[\"reviewText\"]), truncation=True, padding=True)\n",
    "\n",
    "# Create TensorFlow dataset for test data\n",
    "batch_size = 8\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings))).batch(batch_size)\n",
    "\n",
    "# Make predictions on the test dataset\n",
    "predictions = model.predict(test_dataset)\n",
    "predicted_labels = tf.argmax(predictions.logits, axis=1)\n",
    "\n",
    "# Map predicted labels back to original ratings (assuming labels are 0 to 4)\n",
    "predicted_ratings = predicted_labels.numpy() + 1\n",
    "\n",
    "# Add the 'overall' column to test_data_df\n",
    "test_data_df['overall'] = predicted_ratings\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "test_data_df.to_csv('predicted_test.csv', index=False)\n",
    "\n",
    "# Note: Since we don't have ground truth labels for the test data, we can't evaluate F1 score or accuracy in this case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b04e9477-4355-4a2c-b46d-a6287a969f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the predicted_test.csv file\n",
    "predicted_test_df = pd.read_csv('predicted_test.csv')\n",
    "\n",
    "# Extract the 'overall' column as 'predicted'\n",
    "predicted_column = predicted_test_df['overall']\n",
    "\n",
    "# Create a new DataFrame with the 'predicted' column\n",
    "submission_df = pd.DataFrame({'predicted': predicted_column})\n",
    "\n",
    "# Save the new DataFrame to q2_submission.csv without index\n",
    "submission_df.to_csv('q2_submission.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379c5a7f-3967-4ff0-be60-e8941474b1c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
